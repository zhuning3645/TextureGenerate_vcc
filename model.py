import torch
import pytorch_lightning as pl
import os
from typing import Optional, Tuple, List, Union
import torch
import numpy as np
from torchvision.utils import make_grid, save_image
from torchvision import transforms
from PIL import Image, ImageOps
from torch.nn.functional import interpolate
import torch.nn.functional as F
from diffusers import ControlNetModel, AutoencoderKL, UniPCMultistepScheduler, DDPMScheduler
# from diffusers.models.attention_processor import IPAdapterAttnProcessor2_0 as IPAttnProcessor, AttnProcessor2_0 as AttnProcessor
import torch.nn as nn

dependencies = ["torch", "numpy", "diffusers", "PIL"]

from stablenormal.pipeline_yoso_normal import YOSONormalsPipeline
from stablenormal.pipeline_stablenormal import StableNormalPipeline
from stablenormal.scheduler.heuristics_ddimsampler import HEURI_DDIMScheduler
import warnings
import math
from itertools import chain
import logging
from src.attention_processor import IPAttnProcessor2_0 as IPAttnProcessor, AttnProcessor2_0 as AttnProcessor


def batch_pad_to_square(images: torch.Tensor) -> Tuple[torch.Tensor, Tuple[int, int]]:
    """ä¼˜åŒ–çš„æ‰¹é‡å¡«å……æ–¹æ³•"""
    B, C, H, W = images.shape
    max_size = max(H, W)
    
    # è®¡ç®—å¯¹ç§°å¡«å……é‡
    pad_h = (max_size - H) // 2, (max_size - H + 1) // 2
    pad_w = (max_size - W) // 2, (max_size - W + 1) // 2
    
    # ä½¿ç”¨é«˜æ•ˆå¡«å……
    return F.pad(images, (pad_w[0], pad_w[1], pad_h[0], pad_h[1])), (H, W)

def batch_resize(images: torch.Tensor, resolution: int) -> torch.Tensor:
    """æ‰¹é‡è°ƒæ•´å¤§å°å¹¶ä¿æŒå®½é«˜æ¯” (B, C, H, W)"""
    B, C, H, W = images.shape
    scale = resolution / min(H, W)
    
    # å‘é‡åŒ–è®¡ç®—æ–°å°ºå¯¸
    new_H = (H * scale / 64).int() * 64
    new_W = (W * scale / 64).int() * 64
    
    # ä½¿ç”¨ç»„åˆæ’å€¼æ–¹æ³•
    return F.interpolate(
        images,
        size=(new_H.item(), new_W.item()),
        mode='bicubic',
        align_corners=False,
        antialias=True
    )

def center_crop(image: torch.Tensor) -> Tuple[torch.Tensor, Tuple[int, int], Tuple[int, int, int, int]]:
    """Crop the center of the tensor to make it square (C, H, W)."""
    _, h, w = image.shape
    crop_size = min(h, w)
    
    top = (h - crop_size) // 2
    left = (w - crop_size) // 2
    
    cropped = image[:, top:top+crop_size, left:left+crop_size]
    return cropped, (h, w), (left, top, left+crop_size, top+crop_size)

def scale_latents(latents):
    latents = (latents - 0.22) * 0.75
    return latents

def unscale_latents(latents):
    latents = latents / 0.75 + 0.22
    return latents

def scale_image(image):
    image = image * 0.5 / 0.8
    return image

def unscale_image(image):
    image = image / 0.5 * 0.8
    return image

def extract_into_tensor(a, t, x_shape):
    b, *_ = t.shape
    out = a.gather(-1, t)
    return out.reshape(b, *((1,) * (len(x_shape) - 1)))

# å›¾åƒæŠ•å½±æ¨¡å‹ï¼šå°†CLIPå›¾åƒç‰¹å¾æ˜ å°„ä¸ºæç¤ºè¯åºåˆ—
# è¾“å…¥å½¢çŠ¶[bs clip_embed_dim] è¾“å‡ºå½¢çŠ¶ [bs, num_tokens, cross_attention_dim]
class ImageProjModel(torch.nn.Module):
    def __init__(self, clip_embed_dim=1024, cross_attention_dim=1024, num_tokens=1024):
        super().__init__()
        self.cross_attention_dim = cross_attention_dim
        self.num_tokens = num_tokens

        self.generator = None

        self.proj = torch.nn.Linear(clip_embed_dim, cross_attention_dim)
        self.norm = torch.nn.LayerNorm(cross_attention_dim)
 
    def forward(self, x):
        # image_embeds: [B, clip_embed_dim]
        assert not torch.isnan(x).any(), "è¾“å…¥åŒ…å«NaN"

        b, c, h, w = x.shape

        # 1. å±•å¹³ç©ºé—´ç»´åº¦: [b, 1024, 2304]
        x = x.reshape(b, c, h * w)

        # 2. äº¤æ¢ç»´åº¦å’Œåˆå¹¶Batch: [b*2304, 1024]
        x = x.permute(0, 2, 1)  # [b, 2304, 1024]
        x = x.reshape(-1, c)    # [b*2304, 1024]

        # 3. çº¿æ€§æ˜ å°„: [b*2304, cross_attention_dim]
        x = self.proj(x)
        x = x.reshape(b, h * w, -1)  # [b, 2304, cross_attention_dim]

        # 5. å½’ä¸€åŒ–
        x = self.norm(x)
        return x
    
class AdapterModule(torch.nn.Module):
    def __init__(self, unet, adapter_dim=64):
        super().__init__()
        # ä¸ºUNetçš„æ¯ä¸ªäº¤å‰æ³¨æ„åŠ›å±‚æ·»åŠ é€‚é…å±‚
        self.adapters = torch.nn.ModuleList()
        
        for block in unet.down_blocks + unet.up_blocks:
            if hasattr(block, "attentions"):
                for attn in block.attentions:
                    for transformer_block in attn.transformer_blocks:

                        # æ³¨å…¥é€‚é…å±‚åˆ°äº¤å‰æ³¨æ„åŠ›æ¨¡å—
                        adapter = torch.nn.Sequential(
                            torch.nn.LayerNorm(adapter_dim),
                            torch.nn.Linear(transformer_block.attn2.to_k.in_features, adapter_dim),
                            torch.nn.ReLU(),
                            torch.nn.Linear(adapter_dim, transformer_block.attn2.to_k.in_features)
                        )
                        self.adapters.append(adapter)
                        # å†»ç»“åŸå§‹æƒé‡ï¼Œåªè®­ç»ƒé€‚é…å™¨
                        for param in transformer_block.attn2.parameters():
                            param.requires_grad = False
                        

class IPAdapter(torch.nn.Module):
    """IP-Adapter"""
    def __init__(self, unet, image_proj_model, adapter_modules, ckpt_path=None):
        super().__init__()
        self.unet = unet
        self.image_proj_model = image_proj_model
        self.adapter_modules = adapter_modules

        if ckpt_path is not None:
            self.load_from_checkpoint(ckpt_path)

    # def forward(self, noisy_latents, timesteps, encoder_hidden_states, image_embeds):
    #     ip_tokens = self.image_proj_model(image_embeds)
    #     encoder_hidden_states = torch.cat([encoder_hidden_states, ip_tokens], dim=1)
    #     # Predict the noise residual
    #     noise_pred = self.unet(noisy_latents, timesteps, encoder_hidden_states).sample
    #     return noise_pred

    def load_from_checkpoint(self, ckpt_path: str):
        # Calculate original checksums
        orig_ip_proj_sum = torch.sum(torch.stack([torch.sum(p) for p in self.image_proj_model.parameters()]))
        orig_adapter_sum = torch.sum(torch.stack([torch.sum(p) for p in self.adapter_modules.parameters()]))

        state_dict = torch.load(ckpt_path, map_location="cpu")

        # Load state dict for image_proj_model and adapter_modules
        self.image_proj_model.load_state_dict(state_dict["image_proj"], strict=True)
        self.adapter_modules.load_state_dict(state_dict["ip_adapter"], strict=True)

        # Calculate new checksums
        new_ip_proj_sum = torch.sum(torch.stack([torch.sum(p) for p in self.image_proj_model.parameters()]))
        new_adapter_sum = torch.sum(torch.stack([torch.sum(p) for p in self.adapter_modules.parameters()]))

        # Verify if the weights have changed
        assert orig_ip_proj_sum != new_ip_proj_sum, "Weights of image_proj_model did not change!"
        assert orig_adapter_sum != new_adapter_sum, "Weights of adapter_modules did not change!"

        print(f"Successfully loaded weights from checkpoint {ckpt_path}")

# åˆ›å»º LightningModule ä½œä¸ºå°è£…å™¨
class WrapperLightningModule(pl.LightningModule):
    def __init__(
        self, 
        local_cache_dir: Optional[str] = None, 
        ckpt_path:Optional[str] = None,
        device="cuda:0", 
        yoso_version='yoso-normal-v0-3', 
        diffusion_version='stable-normal-v0-1',
        drop_cond_prob = 0.05,
        prompt="The normal map",

        
        # ip_adpter_weight: Optional[str] = "ip-adapter_sd15_fixed.safetensors",
    ):
        super().__init__()

        logging.getLogger("diffusers.models.attention_processor").setLevel(logging.ERROR)  

        # åˆå§‹åŒ–é…ç½®å‚æ•°
        self.local_cache_dir = local_cache_dir
        self.yoso_version = yoso_version
        self.diffusion_version = diffusion_version
        # self.ip_adpter_weight = ip_adpter_weight
        self.num_timesteps = 500
        self.drop_cond_prob = drop_cond_prob
        self.automatic_optimization = True
        self.prompt = prompt
        self.prompt_embeds = None
        torch.autograd.set_detect_anomaly(True)
        # æ·»åŠ å†…å­˜ä¼˜åŒ–é…ç½®
        self.batch_format_ready = False
        self.best_loss = float('inf')
        
        # ç¼“å­˜é¢„å¤„ç†ç»“æœ
        self.preprocessed_cache = {}
        
        # è‡ªåŠ¨æ··åˆç²¾åº¦é…ç½®
        self.autocast_enabled = True
        self.scaler = torch.cuda.amp.GradScaler(enabled=True)
    

        # åˆå§‹åŒ–æ¨¡å‹ç»„ä»¶
        self.x_start_pipeline = YOSONormalsPipeline.from_pretrained(
            os.path.join(local_cache_dir, yoso_version),
            trust_remote_code=True,
            safety_checker=None,
            variant="fp16",
            torch_dtype=torch.float16
        ).to(device)
        
        self.pipe = StableNormalPipeline.from_pretrained(
            os.path.join(local_cache_dir, diffusion_version),
            trust_remote_code=True,
            safety_checker=None,
            variant="fp16",
            torch_dtype=torch.float16,
            scheduler=HEURI_DDIMScheduler(
                prediction_type='sample',
                beta_start=0.00085,
                beta_end=0.0120,
                beta_schedule="scaled_linear"
            ),
            low_cpu_mem_usage=False,
            ignore_mismatched_sizes=True
        )
        original_conv_in = self.pipe.unet.conv_in
        self.conv_in = nn.Conv2d(8, 320, kernel_size=3, padding=1, dtype=torch.float16)

        # åˆ›å»ºæ–°çš„ conv_in å±‚ï¼ˆè¾“å…¥é€šé“æ”¹ä¸º 8ï¼Œå…¶ä»–å‚æ•°ä¸å˜ï¼‰
        new_conv_in = nn.Conv2d(
            in_channels=8,  # ä¿®æ”¹è¾“å…¥é€šé“ä¸º 8
            out_channels=original_conv_in.out_channels,
            kernel_size=original_conv_in.kernel_size,
            stride=original_conv_in.stride,
            padding=original_conv_in.padding,
            dilation=original_conv_in.dilation,
            groups=original_conv_in.groups,
            bias=original_conv_in.bias is not None,
            padding_mode=original_conv_in.padding_mode
        )

        # æƒé‡å¤åˆ¶é€»è¾‘ï¼ˆæ ¸å¿ƒä¿®æ”¹éƒ¨åˆ†ï¼‰
        with torch.no_grad():
            # è·å–åŸå§‹æƒé‡ [out_channels=320, in_channels=4, kH, kW]
            original_weight = original_conv_in.weight
            
            # æ²¿è¾“å…¥é€šé“ç»´åº¦å¤åˆ¶åŸå§‹æƒé‡ï¼ˆæ‰©å±•ä¸º8é€šé“ï¼‰
            # è¿™é‡Œå°†å‰4é€šé“å¤åˆ¶åˆ°å4é€šé“
            duplicated_weight = torch.cat([
                original_weight, 
                original_weight   # å¤åˆ¶å¹¶å‡å¼±æ–°é€šé“æƒé‡
            ], dim=1)  # æ²¿è¾“å…¥é€šé“ç»´åº¦æ‹¼æ¥
            
            # è°ƒæ•´æƒé‡å½¢çŠ¶ä»¥åŒ¹é…æ–°å·ç§¯å±‚ [320, 8, 3, 3]
            new_conv_in.weight[:] = duplicated_weight
            
            # å¤åˆ¶åç½®ï¼ˆè‹¥æœ‰ï¼‰
            if new_conv_in.bias is not None:
                new_conv_in.bias.data = original_conv_in.bias.data.clone()


        # æ›¿æ¢ UNet çš„ conv_in å±‚
        self.pipe.unet.conv_in = new_conv_in

        print(self.pipe.unet.conv_in.weight.shape)  # åº”è¾“å‡º torch.Size([320, 8, 3, 3])
        print(self.pipe.unet.conv_in.weight.dtype)  # åº”è¾“å‡º torch.float16

        train_sched = DDPMScheduler.from_config(self.pipe.scheduler.config)
        # if isinstance(self.pipe.unet, UNet2DConditionModel):
        #     self.pipe.unet = RefOnlyNoisedUNet(self.pipe.unet, train_sched, self.pipe.scheduler)

        self.train_scheduler = train_sched
        # self.unet = self.pipe.unet

        #åˆå§‹åŒ–pipeline
        self.x_start_pipeline.to(device)
        self.pipe.to(device)

        self.sqrt_alphas_cumprod = torch.sqrt(self.pipe.scheduler.alphas_cumprod).to(device='cuda:0')
        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.pipe.scheduler.alphas_cumprod).to(device='cuda:0')

        #å†»ç»“ä¸æ›´æ–°å‚æ•°çš„ç»„ä»¶
        self.pipe.unet.requires_grad_(False)
        self.pipe.vae.requires_grad_(False)
        self.sqrt_alphas_cumprod.requires_grad_(False)
        self.sqrt_one_minus_alphas_cumprod.requires_grad_(False)

        # # åœ¨ç±»åˆå§‹åŒ–æˆ–ä»£ç å¼€å§‹å¤„æ·»åŠ 
        # warnings.filterwarnings("ignore", 
        #     message=".*cross_attention_kwargs.*are not expected by.*and will be ignored.*")

        # åŠ è½½IPé€‚é…å™¨
        # self.pipe.load_ip_adapter(
        #     "/data/shared/TextureGeneration/IP-Adapter/IP-Adapter",
        #     subfolder="models",
        #     weight_name=ip_adpter_weight,
        #     local_files_only=True,
        #     low_cpu_mem_usage=False
        # )

        # image_proj_model = ImageProjModel(
        #     cross_attention_dim=self.pipe.unet.config.cross_attention_dim,
        #     clip_embed_dim=1024,
        #     num_tokens=4,
        # ).float().cuda().to(device)
        image_proj_model = ImageProjModel().cuda()
        
        # init adapter modules
        attn_procs = {}
        unet_sd = self.pipe.unet.state_dict()
        for name in self.pipe.unet.attn_processors.keys():
            cross_attention_dim = None if name.endswith("attn1.processor") else self.pipe.unet.config.cross_attention_dim
            if name.startswith("mid_block"):
                hidden_size = self.pipe.unet.config.block_out_channels[-1]
            elif name.startswith("up_blocks"):
                block_id = int(name[len("up_blocks.")])
                hidden_size = list(reversed(self.pipe.unet.config.block_out_channels))[block_id]
            elif name.startswith("down_blocks"):
                block_id = int(name[len("down_blocks.")])
                hidden_size = self.pipe.unet.config.block_out_channels[block_id]
            if cross_attention_dim is None:
                attn_procs[name] = AttnProcessor()
            else:
                # layer_name = name.split(".processor")[0]
                # weights = {
                #     "to_k_ip.0.weight": unet_sd.get(layer_name + ".to_k.0.weight", unet_sd[layer_name + ".to_k.weight"]),
                #     "to_v_ip.0.weight": unet_sd.get(layer_name + ".to_v.0.weight", unet_sd[layer_name + ".to_v.weight"]),
                # }
                # attn_procs[name] = IPAttnProcessor(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim)
                # ip_processor = IPAttnProcessor(
                #         hidden_size=hidden_size,
                #         cross_attention_dim=cross_attention_dim,
                #         num_tokens=[4],  # å…¼å®¹ num_tokens=4
                #         scale=[1.0]      # å…¼å®¹ scale=1.0
                #     )
                # # åŠ è½½æƒé‡
                # ip_processor.to_k_ip[0].weight = torch.nn.Parameter(weights["to_k_ip.0.weight"])
                # ip_processor.to_v_ip[0].weight = torch.nn.Parameter(weights["to_v_ip.0.weight"])
                # attn_procs[name] = ip_processor
                layer_name = name.split(".processor")[0]
                weights = {
                    "to_k_ip.weight": unet_sd[layer_name + ".to_k.weight"],
                    "to_v_ip.weight": unet_sd[layer_name + ".to_v.weight"],
                }
                attn_procs[name] = IPAttnProcessor(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim)
                attn_procs[name].load_state_dict(weights)
        
        self.pipe.unet.set_attn_processor(attn_procs)
        adapter_modules = torch.nn.ModuleList(self.pipe.unet.attn_processors.values()).to(device)

        # attn_processors = self.pipe.unet.attn_processors.values()
        # wrapped_processors = [AttnProcessorWrapper(proc) if not isinstance(proc, torch.nn.Module) else proc for proc in attn_processors]
        # adapter_modules = torch.nn.ModuleList(wrapped_processors).to(device)
        
        ip_adapter = IPAdapter(self.pipe.unet, image_proj_model, adapter_modules, None)

        # æ­¥éª¤2: åˆ›å»ºIPAdapterå®ä¾‹
        self.image_proj_model=ip_adapter.image_proj_model
        self.adapter_modules=ip_adapter.adapter_modules

        self.image_proj_model.requires_grad_(True)
        self.adapter_modules.requires_grad_(True)
        self.conv_in.requires_grad_(True)
            
        # æ—¥å¿—ç›®å½•
        self.log_dir = "training_logs"
        os.makedirs(self.log_dir, exist_ok=True)

        #ç”Ÿæˆæ–‡æœ¬åµŒå…¥
        num_images_per_prompt = 1
        with torch.no_grad():
            if self.prompt_embeds is None:
                prompt_embeds, negative_prompt_embeds = self.pipe.encode_prompt(
                    self.prompt,
                    device,
                    num_images_per_prompt,
                    False,
                    prompt_embeds=self.prompt_embeds,
                    negative_prompt_embeds=None,
                    lora_scale=None,
                    clip_skip=None,
                )
                self.prompt_embeds = prompt_embeds
        

        # éªŒè¯å¯è®­ç»ƒå‚æ•°æ•°é‡
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"Trainable parameters: {trainable_params}")  # åº”å¤§äº0

        print("Modified structure:", self.pipe.unet.encoder_hid_proj)


    
    def _preprocess_image(self, size: int, images: Union[List[Image.Image], torch.Tensor]) -> torch.Tensor:
        """æ‰¹é‡é¢„å¤„ç†ç®¡é“ï¼ˆå…¼å®¹PILå’ŒTensorè¾“å…¥ï¼‰"""
        # è¾“å…¥æ ¼å¼éªŒè¯
        if isinstance(images, list):
            # æƒ…å†µ1ï¼šè¾“å…¥ä¸ºPILå›¾åƒåˆ—è¡¨
            tensor_batch = torch.stack([
                transforms.functional.to_tensor(img.convert('RGB')) 
                for img in images
            ]).to(self.device, non_blocking=True)
        elif isinstance(images, torch.Tensor):
            # æƒ…å†µ2ï¼šè¾“å…¥å·²ç»æ˜¯é¢„å¤„ç†è¿‡çš„Tensor
            if images.dim() == 3:
                tensor_batch = images.unsqueeze(0).to(self.device)
            elif images.dim() == 4:
                tensor_batch = images.to(self.device)
            else:
                raise ValueError(f"Invalid tensor dimensions: {images.shape}")
        else:
            raise TypeError(f"Unsupported input type: {type(images)}")

        # è‡ªåŠ¨æ··åˆç²¾åº¦ä¼˜åŒ–
        with torch.cuda.amp.autocast(enabled=self.autocast_enabled):
            # æ‰¹é‡å¡«å…… (B, C, H, W)
            padded, _ = batch_pad_to_square(tensor_batch)
            
            # å¸¦åé”¯é½¿çš„è°ƒæ•´å¤§å°
            resized = F.interpolate(
                padded,
                size=(size, size),
                mode='bicubic',
                align_corners=False,
                antialias=True
            )
            
        # å½’ä¸€åŒ–åˆ°[-1, 1]èŒƒå›´
        return resized.mul(2).sub(1)
    

    def _optimize_batch_format(self):
        """ä¼˜åŒ–æ‰¹é‡å¤„ç†é…ç½®"""
        torch.backends.cudnn.benchmark = True
        torch.autograd.profiler.profile(False)
        torch.autograd.profiler.emit_nvtx(False)
        self.batch_format_ready = True
    

    def prepare_batch_data(self, batch):
        """
        è¾“å…¥æ•°æ®æ ¼å¼:
        batch = {
            'input_images': PILå›¾åƒåˆ—è¡¨,
            'ref_normal': æç¤ºå›¾åƒåˆ—è¡¨,
            'render_gt':ç›®æ ‡å›¾åƒåˆ—è¡¨
        }
        """
        lrm_generator_input = {}
        render_gt = {}   # for supervision
        processed = {}
        

        images = self._preprocess_image(
            size=768, 
            images=batch['input_images']  # ç›´æ¥ä¼ å…¥æ•´ä¸ªå›¾åƒåˆ—è¡¨
        ).to(self.device)
        
        size_crop = 512
        # å‚è€ƒæ³•çº¿å›¾
        if 'ref_normals' in batch and batch['ref_normals'] is not None:
            ref_normal = self._preprocess_image(
                size=size_crop,
                images=batch['ref_normals']  # ç›´æ¥ä¼ é€’æ•´ä¸ªbatch
            ).to(self.device)
        else:
            ref_normal = None


        # å¤„ç† render_gt å­—æ®µ
        if 'render_gt' in batch and batch['render_gt'] is not None:
            _render_gt = batch['render_gt']  # ç›´æ¥ä½¿ç”¨åŸå§‹å€¼
            render_gt = _render_gt.to(self.device)
        else:
            render_gt = None


        lrm_generator_input['images'] = images.to(self.device)
        lrm_generator_input['ref_normals'] = ref_normal.to(self.device)

        image_resolution = 768
        init_latents = torch.zeros([1, 4, image_resolution // 8, image_resolution // 8], 
                                    device="cuda", dtype=torch.float16)
        lrm_generator_input['init_latents'] = init_latents.to(self.device)


        return lrm_generator_input, render_gt
    

    
    def training_step(self, batch, batch_idx):

        with torch.cuda.amp.autocast(enabled=self.autocast_enabled):
            model_inputs, render_gt = self.prepare_batch_data(batch)

            image = model_inputs['images']
            ref_normal = model_inputs['ref_normals']

            image = image.squeeze(1)  # 1 3 768 768
            ref_normal = ref_normal.squeeze(1)
            render_gt = render_gt.squeeze(1)

            # sample random timestep
            B = image.shape[0]
            t = torch.randint(0, self.num_timesteps, size=(B,)).long().to(self.device)

            '''
            encode_condition_imageä½¿ç”¨é‡‡æ ·ï¼Œç”Ÿæˆå¸¦éšæœºæ€§latentsï¼Œç”¨äºè®­ç»ƒï¼Œå¢å¼ºé²æ£’æ€§
            prepare_latentsä½¿ç”¨å‡å€¼ï¼Œé€‚åˆç”¨äºæ¨ç†é˜¶æ®µï¼Œèƒ½ç”Ÿæˆç¡®å®šæ€§çš„latents
            '''

            latents, generator = None, None

            # classifier-free guidance
            if np.random.rand() < self.drop_cond_prob:
                cond_latents = self.encode_condition_image(torch.zeros_like(image))# [1, 4, 96, 96]
            else:
                cond_latents = self.encode_condition_image(image)


            latents_gt = self.encode_target_images(render_gt)
            noise = torch.randn_like(latents_gt)
            latents_noisy = self.train_scheduler.add_noise(latents_gt, noise, t)
            
            #å¯¹ref_normalä½¿ç”¨dino encoder ç”Ÿæˆfeatures
            ref_normal = ref_normal.to(dtype=next(self.pipe.prior.parameters()).dtype) # bs 3 512 512
            # print(f"ref_normal:{ref_normal.shape}")
            dino_features_ref = self.pipe.prior(ref_normal)

            dino_features_ref = dino_features_ref.type(torch.float16) # bs 1024 48 48 
            # print(f"dino_features_ref:{dino_features_ref.shape}")

            #ä½¿ç”¨dino encoderçš„ç‰¹å¾ç”Ÿæˆtokens
            ip_tokens = self.image_proj_model(dino_features_ref) # bs 48*48 1024
            # print(f"ip_tokens:{ip_tokens.shape}")
            ip_tokens = ip_tokens.half()
            batch_size = ip_tokens.size(0)
            # æ ¹æ®bsæ‰©å±•æ–‡æœ¬åµŒå…¥çš„ç»´åº¦
            self.prompt_embeds = self.prompt_embeds.expand(batch_size, -1, -1) # bs 77 1024
            encoder_hidden_states = torch.cat([self.prompt_embeds, ip_tokens], dim=1) # bs 77 1024 + bs 2304 1024

            # å†…å­˜ä¼˜åŒ–ï¼šé‡Šæ”¾ä¸éœ€è¦çš„ç¼“å­˜
            torch.cuda.empty_cache()

            v_target = self.get_v(latents_gt, noise, t).half()
            
            # ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ 
            prev_noise = self.forward(latents_noisy, cond_latents, t, encoder_hidden_states)
            loss, loss_dict = self.compute_loss(prev_noise, v_target)

            # æ–°å¢ä»£ç ï¼šè®°å½•æœ€å°losså€¼
            current_loss = loss.item()  # å°†lossè½¬ä¸ºPythonæ•°å€¼
            if current_loss < self.best_loss:
                old_best_loss = self.best_loss  
                self.best_loss = current_loss
                print(f"ğŸ™ğŸ™ğŸ™ oh!!! New minimum loss achieved: {old_best_loss:.4f} --> {self.best_loss:.4f}")
                print(f"(â†“{old_best_loss - self.best_loss:.4f})")

            # æ—¥å¿—è®°å½•
            self.log_dict(loss_dict, prog_bar=True, logger=True, on_step=True, on_epoch=True)
            
            self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)

            lr = self.optimizers().param_groups[0]['lr']
            self.log('lr_abs', lr, prog_bar=True, logger=True, on_step=True, on_epoch=False)

        #å¯è§†åŒ–ä¿å­˜ï¼ˆæ¯100 * accumulate_grad_batchesæ­¥ï¼‰
        if self.global_step % 25 == 0 and self.global_rank == 0:
            self._save_training_samples(latents_noisy, t, prev_noise, render_gt, input_image = image, ref_image = ref_normal)
            
        return loss

    
    def forward(self, latents_noisy, cond_latents, t, encoder_hidden_states):
        """ä¼˜åŒ–çš„å‰å‘ä¼ æ’­"""

        latent = torch.cat([latents_noisy, cond_latents], dim=1)
        # print(f"latent:{latent.shape}")

        prev_noise = self.pipe.dino_unet_forward(
            self.pipe.unet,
            latent,
            t,
            encoder_hidden_states = encoder_hidden_states,
            return_dict = False,
        )[0]

        return prev_noise


    def compute_loss(self, noise_pred, noise_gt):

        # 1. è®¡ç®—MSEæŸå¤±ï¼ˆæ¨¡å‹é¢„æµ‹çš„å™ªå£° vs çœŸå®å™ªå£°ï¼‰
        loss = F.mse_loss(noise_pred, noise_gt)
        
        prefix = 'train'
        loss_dict = {}
        loss_dict.update({f'{prefix}/loss':loss})
        
        return loss, loss_dict


    @torch.no_grad
    def validation_step(self, batch, batch_idx):
        # å‡†å¤‡éªŒè¯æ•°æ®
        lrm_generator_input, render_gt = self.prepare_batch_data(batch)
        
        # ç”ŸæˆéªŒè¯ç»“æœ
        # with torch.no_grad():
        #     render_out = self.forward(lrm_generator)

        render_images = render_gt['render_gt']
        self.validation_step_outputs.append(render_images)
        # ä¿å­˜ç»“æœ
        self._save_validation_results(render_images, batch)


    @torch.no_grad
    def _save_training_samples(self, latents_noisy, t, prev_noise, render_gt, input_image, ref_image):
        """ä¿å­˜è®­ç»ƒè¿‡ç¨‹æ ·æœ¬"""
        #é¢„æµ‹èµ·å§‹latentsã€æ—¶é—´æ­¥ã€é¢„æµ‹å™ªå£°
        latents_pred = self.predict_start_from_z_and_v(latents_noisy, t, prev_noise)

        # ä½¿ç”¨è®­ç»ƒç½‘ç»œé¢„æµ‹çš„å™ªå£°ç»™å·²ç»åŠ å™ªçš„latentsé™å™ª
        latents = unscale_latents(latents_pred)
        # å°† scaling_factor è½¬æ¢ä¸ºå¼ é‡ï¼Œå¹¶ç¡®ä¿ç±»å‹å’Œè®¾å¤‡åŒ¹é…
        self.pipe.vae.config.scaling_factor = torch.tensor(
            self.pipe.vae.config.scaling_factor,
            dtype=torch.float16,  # å¼ºåˆ¶ä½¿ç”¨ float16
            device=latents.device
        )

        # ç¡®ä¿ latents æ˜¯ float16
        latents = latents.to(torch.float16)

        prediction = unscale_image(self.pipe.decode_prediction(latents))
        images = (prediction.clip(-1, 1) + 1) / 2

        # images = unscale_image(self.pipe.vae.decode(latents / self.pipe.vae.config.scaling_factor, return_dict=False)[0])   # [-1, 1]
        # images = (images * 0.5 + 0.5).clamp(0, 1)
        ref_image = F.interpolate(ref_image, size=(768, 768), mode='bilinear')
        group1 = torch.cat([ref_image, input_image], dim=-1) 
        group2 = torch.cat([render_gt, images], dim=-1)
        images = torch.cat([group1, group2], dim=-2)

        grid = make_grid(images, nrow=images.shape[0], normalize=True, value_range=(0, 1))

        save_path = os.path.join(self.logdir, 'images', f"*train_step_{self.global_step:07d}.png")
        save_dir = os.path.join(self.logdir, 'images')
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        save_image(grid, save_path, normalize=True)
        


    def _save_validation_results(self, output, batch):
        """ä¿å­˜éªŒè¯ç»“æœ"""
        val_dir = os.path.join(self.log_dir, "validation")
        os.makedirs(val_dir, exist_ok=True)
        
        for i, (img, normal) in enumerate(zip(batch['input_images'], output['generated_normals'])):
            save_image(normal, os.path.join(val_dir, f"val_{self.current_epoch}_{i}.png"))


    def encode_condition_image(self, images):
        dtype = next(self.pipe.vae.parameters()).dtype
        
        # ç¡¬ç¼–ç CLIPæ ‡å‡†å½’ä¸€åŒ–å‚æ•°
        clip_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073], 
                            device=self.device).view(1, 3, 1, 1)
        clip_std = torch.tensor([0.26862954, 0.26130258, 0.27577711],
                            device=self.device).view(1, 3, 1, 1)
        
        # ç›´æ¥å¤„ç†å·²ç»é¢„å¤„ç†å¥½çš„å¼ é‡
        # å‡è®¾è¾“å…¥imagesæ»¡è¶³ï¼š
        # 1. å·²ç»æ˜¯RGBä¸‰é€šé“ (æ— alphaé€šé“)
        # 2. å°ºå¯¸å·²è°ƒæ•´ä¸º224x224
        # 3. æ•°å€¼èŒƒå›´åœ¨[0,1]ä¹‹é—´
        
        # æ‰§è¡ŒCLIPç‰¹æœ‰çš„å½’ä¸€åŒ–
        normalized = (images.to(self.device) - clip_mean) / clip_std
        
        # ç¼–ç ä¸ºæ½œåœ¨ç©ºé—´
        latents = self.pipe.vae.encode(normalized.to(dtype)).latent_dist.sample()
        return latents

    def _get_resample_mode(self, resample_id: int) -> str:
        """å°†resampleå‚æ•°è½¬æ¢ä¸ºPyTorchæ’å€¼æ¨¡å¼"""
        resample_map = {
            0: "nearest",
            2: "bilinear",
            3: "bicubic"
        }
        return resample_map.get(resample_id, "bicubic")

    def configure_optimizers(self):
        # æ·»åŠ è‡ªåŠ¨å­¦ä¹ ç‡ç¼©æ”¾
        base_lr = self.learning_rate
        optimizer = torch.optim.AdamW(
            params=chain(
                self.image_proj_model.parameters(),
                self.adapter_modules.parameters(),
                ),
            lr=base_lr,
            weight_decay=1e-2,
            fused=True  # å¯ç”¨èåˆä¼˜åŒ–å™¨
        )
        
        # åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
        scheduler = {
            'scheduler': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
                optimizer,
                T_0 = 500,
                T_mult = 2,
                eta_min = base_lr / 4,
            ),
            'interval': 'step'
        }
        return [optimizer], [scheduler]
    



    @torch.no_grad()
    def encode_target_images(self, images):
        dtype = next(self.pipe.vae.parameters()).dtype
        # equals to scaling images to [-1, 1] first and then call scale_image
        images = (images - 0.5) / 0.8   # [-0.625, 0.625]
        posterior = self.pipe.vae.encode(images.to(dtype)).latent_dist
        latents = posterior.sample() * self.pipe.vae.config.scaling_factor
        latents = scale_latents(latents)
        return latents
    

    def extract_into_tensor(a, t, x_shape):
            b, *_ = t.shape
            out = a.gather(-1, t)
            return out.reshape(b, *((1,) * (len(x_shape) - 1)))


    def get_v(self, x, noise, t):
        return (
            extract_into_tensor(self.sqrt_alphas_cumprod, t, x.shape) * noise -
            extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x.shape) * x
        )


    def predict_start_from_z_and_v(self, x_t, t, v):
        return (
            extract_into_tensor(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t -
            extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v
        )
    
    
